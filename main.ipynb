{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c64fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def join_products_categories(\n",
    "    df_products: DataFrame,\n",
    "    df_categories: DataFrame,\n",
    "    df_product_category: DataFrame,\n",
    "    product_id_col: str = \"product_id\",\n",
    "    product_name_col: str = \"product_name\",\n",
    "    category_id_col: str = \"category_id\",\n",
    "    category_name_col: str = \"category_name\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"Join products and categories\"\"\"\n",
    "\n",
    "    p = df_products.alias(\"p\")\n",
    "    c = df_categories.alias(\"c\")\n",
    "    pc = df_product_category.dropDuplicates([\"product_id\",\"category_id\"]).alias(\"pc\")\n",
    "\n",
    "    joined = (\n",
    "        p.join(pc, col(f\"p.{product_id_col}\") == col(f\"pc.{product_id_col}\"), \"left\")\n",
    "         .join(c, col(f\"pc.{category_id_col}\") == col(f\"c.{category_id_col}\"), \"left\")\n",
    "         .select(\n",
    "             col(f\"p.{product_name_col}\").alias(\"product_name\"),\n",
    "             col(f\"c.{category_name_col}\").alias(\"category_name\"),\n",
    "         )\n",
    "    )\n",
    "    return joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e0736e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/08 03:10:56 WARN Utils: Your hostname, DESKTOP-EL02S3D, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/09/08 03:10:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/08 03:10:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|product_name|category_name|\n",
      "+------------+-------------+\n",
      "|     AirPods|  Accessories|\n",
      "|      iPhone|  Accessories|\n",
      "|     MacBook|    Computers|\n",
      "|        iPad|  Electronics|\n",
      "|      iPhone|  Electronics|\n",
      "|     Sticker|         NULL|\n",
      "|   Trash Can|         NULL|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "products_data = [\n",
    "    (1, \"iPhone\"),\n",
    "    (2, \"MacBook\"),\n",
    "    (3, \"AirPods\"),\n",
    "    (4, \"Sticker\"),  # no category\n",
    "    (5, \"iPad\"),\n",
    "    (6, \"Trash Can\"), # no category\n",
    "]\n",
    "\n",
    "products_cols = [\"product_id\", \"product_name\"]\n",
    "\n",
    "df_products = spark.createDataFrame(products_data, products_cols)\n",
    "\n",
    "categories_data = [\n",
    "    (1, \"Electronics\"),\n",
    "    (2, \"Computers\"),\n",
    "    (3, \"Accessories\"),\n",
    "    (4, \"Cars\"), # no products\n",
    "]\n",
    "categories_cols = [\"category_id\", \"category_name\"]\n",
    "\n",
    "df_categories = spark.createDataFrame(categories_data, categories_cols)\n",
    "\n",
    "product_category_data = [\n",
    "    (1, 1),\n",
    "    (1, 3),\n",
    "    (2, 2),\n",
    "    (3, 3),\n",
    "    (5, 1),\n",
    "    (1, 1),\n",
    "]\n",
    "\n",
    "product_category_cols = [\"product_id\", \"category_id\"]\n",
    "\n",
    "df_product_category = spark.createDataFrame(product_category_data, product_category_cols)\n",
    "\n",
    "df_result = join_products_categories(df_products, df_categories, df_product_category)\n",
    "\n",
    "df_result.orderBy(col(\"category_name\").asc_nulls_last(), col(\"product_name\").asc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ae699f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests initialize\n",
    "import ipytest, pytest\n",
    "__file__ = \"main.ipynb\"\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e131f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixtures\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spark():\n",
    "    spark = (\n",
    "        SparkSession.builder.master(\"local[*]\")\n",
    "        .appName(\"pytest-pyspark\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    yield spark\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f82c5717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform linux -- Python 3.11.2, pytest-8.4.2, pluggy-1.6.0\n",
      "rootdir: /home/light/testovie/pyspark-products\n",
      "configfile: pyproject.toml\n",
      "collected 2 items\n",
      "\n",
      "t_70e2b9a3b4194b95ad4e0a5339f0e726.py "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 03:11:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                     [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 2.88s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tests\n",
    "def test_pairs_and_nulls(spark):\n",
    "    \"\"\"Tests join_products_categories method given eq values\"\"\"\n",
    "\n",
    "    df_products = spark.createDataFrame(\n",
    "        [(1, \"iPhone\"), (2, \"MacBook\"), (3, \"AirPods\"), (4, \"Sticker\"), (5, \"iPad\"), (6, \"Trash Can\")],\n",
    "        [\"product_id\", \"product_name\"],\n",
    "    )\n",
    "    df_categories = spark.createDataFrame(\n",
    "        [(1, \"Electronics\"), (2, \"Computers\"), (3, \"Accessories\"), (4, \"Cars\")],\n",
    "        [\"category_id\", \"category_name\"],\n",
    "    )\n",
    "    df_pc = spark.createDataFrame(\n",
    "        [(1, 1), (1, 3), (2, 2), (3, 3), (5, 1)],\n",
    "        [\"product_id\", \"category_id\"],\n",
    "    )\n",
    "\n",
    "    out = join_products_categories(df_products, df_categories, df_pc)\n",
    "\n",
    "    expected = spark.createDataFrame(\n",
    "        [\n",
    "            (\"AirPods\", \"Accessories\"),\n",
    "            (\"iPhone\", \"Accessories\"),\n",
    "            (\"MacBook\", \"Computers\"),\n",
    "            (\"iPad\", \"Electronics\"),\n",
    "            (\"iPhone\", \"Electronics\"),\n",
    "            (\"Sticker\", None),\n",
    "            (\"Trash Can\", None),\n",
    "        ],\n",
    "        [\"product_name\", \"category_name\"],\n",
    "    )\n",
    "\n",
    "    assert set(map(tuple, out.collect())) == set(map(tuple, expected.collect()))\n",
    "\n",
    "def test_duplicate_relations_drop(spark):\n",
    "    \"\"\"Tests that dup relations are dropped\"\"\"\n",
    "\n",
    "    df_products = spark.createDataFrame([(1, \"USB-C Cable\")], [\"product_id\", \"product_name\"])\n",
    "    df_categories = spark.createDataFrame([(3, \"Accessories\")], [\"category_id\", \"category_name\"])\n",
    "    \n",
    "    df_pc = spark.createDataFrame([(1, 3), (1, 3)], [\"product_id\", \"category_id\"])\n",
    "\n",
    "    out = join_products_categories(df_products, df_categories, df_pc)\n",
    "\n",
    "    rows = out.collect()\n",
    "    assert len(rows) == 1\n",
    "\n",
    "ipytest.run(\"-v\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-products-7CRSksZP-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
